{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/hackernoon/building-a-bigram-hidden-markov-model-for-part-of-speech-tagging-1b784a87ab2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skindanje raznih corpusa s weba\n",
    "#ovo radim jer iz zbog manjka https-a ne radi download\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the training data\n",
    "from nltk.corpus import brown\n",
    "\n",
    "brown_sents = brown.sents(categories = \"news\")\n",
    "\n",
    "#moramo ga konvertirati u listu\n",
    "brown_tagged_sents = list(brown.tagged_sents(categories = \"news\", tagset = \"universal\"))\n",
    "\n",
    "#podjela\n",
    "size = int(0.9 * len(brown_tagged_sents))\n",
    "random.shuffle(brown_tagged_sents)\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m tree \u001b[39m=\u001b[39m AugmentedSuffixTree()\n\u001b[1;32m      6\u001b[0m bigram \u001b[39m=\u001b[39m BigramModel(train_sents)\n\u001b[0;32m----> 8\u001b[0m bigram\u001b[39m.\u001b[39;49mtrain(train_sents)\n",
      "File \u001b[0;32m~/Desktop/ml/rj-dz1/BigramModel.py:40\u001b[0m, in \u001b[0;36mBigramModel.train\u001b[0;34m(self, tagged_train_sents)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mif\u001b[39;00m tag \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagWordCount:\n\u001b[1;32m     39\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagWordCount[tag] \u001b[39m=\u001b[39m {}\n\u001b[0;32m---> 40\u001b[0m \u001b[39mif\u001b[39;00m tag \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtagTransitionCount[prevTag]:\n\u001b[1;32m     41\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagWordCount[tag] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "#feeding our bigram model\n",
    "from BigramModel import BigramModel\n",
    "from AugmentedSuffixTree import AugmentedSuffixTree\n",
    "\n",
    "tree = AugmentedSuffixTree()\n",
    "bigram = BigramModel(train_sents)\n",
    "\n",
    "bigram.train(train_sents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
